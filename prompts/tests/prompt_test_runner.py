#!/usr/bin/env python3
"""
Prompt Test Runner for validating prompt versions against benchmark queries.

This module provides a testing framework to ensure prompt changes don't break
existing functionality and maintain expected behaviors.
"""

import json
import re
import sys
import time
import traceback
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import yaml
from dotenv import load_dotenv
from rich.console import Console
from rich.table import Table
from rich.progress import track

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

# Load environment variables from .env file
load_dotenv(Path(__file__).parent.parent.parent / ".env")

from prompts.prompt_manager import PromptManager
from landuse.agents.landuse_agent import LanduseAgent
from landuse.config.landuse_config import LanduseConfig


@dataclass
class TestResult:
    """Result of a single test case."""
    name: str
    query: str
    passed: bool
    errors: List[str] = field(default_factory=list)
    warnings: List[str] = field(default_factory=list)
    execution_time: float = 0.0
    sql_generated: Optional[str] = None
    response_text: Optional[str] = None
    critical: bool = False

    @property
    def status_emoji(self) -> str:
        """Return emoji based on test status."""
        if self.passed:
            return "✅"
        elif self.critical:
            return "❌"  # Critical failure
        else:
            return "⚠️"  # Non-critical failure


@dataclass
class TestSuiteResult:
    """Result of entire test suite execution."""
    version: str
    total_tests: int
    passed_tests: int
    failed_tests: int
    critical_failures: int
    execution_time: float
    test_results: List[TestResult]
    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())

    @property
    def pass_rate(self) -> float:
        """Calculate pass rate percentage."""
        if self.total_tests == 0:
            return 0.0
        return (self.passed_tests / self.total_tests) * 100

    @property
    def all_critical_passed(self) -> bool:
        """Check if all critical tests passed."""
        return self.critical_failures == 0


class PromptTestRunner:
    """Runner for prompt version testing."""

    def __init__(self, verbose: bool = False):
        """
        Initialize the test runner.

        Args:
            verbose: If True, show detailed output during testing
        """
        self.verbose = verbose
        self.console = Console()
        self.prompt_manager = PromptManager()

    def load_benchmark_queries(self) -> Dict[str, Any]:
        """
        Load benchmark queries from YAML file.

        Returns:
            Dictionary of test categories and queries
        """
        queries_file = Path(__file__).parent / "benchmark_queries.yaml"

        if not queries_file.exists():
            raise FileNotFoundError(f"Benchmark queries file not found: {queries_file}")

        with open(queries_file, 'r') as f:
            return yaml.safe_load(f)

    def extract_sql_from_response(self, agent_state: Dict) -> Optional[str]:
        """
        Extract SQL query from agent response/state.

        Args:
            agent_state: The agent's internal state or response

        Returns:
            Extracted SQL query or None
        """
        # TODO: Implement proper SQL extraction
        # Current limitation: Cannot access the SQL generated by the agent
        # Future improvements needed:
        # 1. Instrument LanduseAgent to expose generated SQL
        # 2. Add callback mechanism to capture intermediate SQL
        # 3. Parse agent state for SQL patterns
        # 4. Consider using LangGraph's state inspection features

        # This is a placeholder - actual implementation needs agent changes
        if isinstance(agent_state, dict):
            for key, value in agent_state.items():
                if 'sql' in key.lower() and value:
                    return str(value)
        return None

    def validate_sql_contains(
        self,
        sql: str,
        expected_contains: List[str],
        expected_not_contains: Optional[List[str]] = None
    ) -> Tuple[bool, List[str]]:
        """
        Validate that SQL contains expected keywords.

        Args:
            sql: Generated SQL query
            expected_contains: List of expected keywords/phrases
            expected_not_contains: List of keywords that should NOT appear

        Returns:
            Tuple of (passed, list of errors)
        """
        errors = []
        sql_upper = sql.upper() if sql else ""

        # Check expected keywords
        for keyword in expected_contains:
            if keyword.upper() not in sql_upper:
                errors.append(f"SQL missing expected keyword: {keyword}")

        # Check forbidden keywords
        if expected_not_contains:
            for keyword in expected_not_contains:
                if keyword.upper() in sql_upper:
                    errors.append(f"SQL contains forbidden keyword: {keyword}")

        return len(errors) == 0, errors

    def validate_response_contains(
        self,
        response: str,
        expected_contains: List[str]
    ) -> Tuple[bool, List[str]]:
        """
        Validate that response contains expected content.

        Args:
            response: Agent's response text
            expected_contains: List of expected words/phrases (can be alternatives)

        Returns:
            Tuple of (passed, list of errors)
        """
        errors = []
        response_lower = response.lower() if response else ""

        for expected in expected_contains:
            # Handle alternative phrases (e.g., ["no data", "not found"])
            if isinstance(expected, list):
                if not any(phrase.lower() in response_lower for phrase in expected):
                    errors.append(f"Response missing any of: {expected}")
            else:
                if expected.lower() not in response_lower:
                    errors.append(f"Response missing expected content: {expected}")

        return len(errors) == 0, errors

    def validate_numeric_range(
        self,
        response: str,
        min_value: Optional[float] = None,
        max_value: Optional[float] = None
    ) -> Tuple[bool, List[str]]:
        """
        Validate that response contains numbers within expected range.

        Args:
            response: Agent's response text
            min_value: Minimum expected value
            max_value: Maximum expected value

        Returns:
            Tuple of (passed, list of errors)
        """
        errors = []

        # Extract numbers from response
        # Look for patterns like "1,234,567" or "1234567" or "1.23 million"
        import re

        # Remove commas and find all numbers
        clean_response = response.replace(',', '')

        # Find all numeric patterns
        number_patterns = [
            r'\d+\.?\d*',  # Regular numbers
            r'\d+\.\d+\s*(?:million|billion|thousand)',  # Numbers with units
        ]

        numbers = []
        for pattern in number_patterns:
            matches = re.findall(pattern, clean_response, re.IGNORECASE)
            for match in matches:
                try:
                    # Handle units
                    if 'million' in match.lower():
                        num = float(re.search(r'[\d.]+', match).group()) * 1000000
                    elif 'billion' in match.lower():
                        num = float(re.search(r'[\d.]+', match).group()) * 1000000000
                    elif 'thousand' in match.lower():
                        num = float(re.search(r'[\d.]+', match).group()) * 1000
                    else:
                        num = float(match)

                    # Only consider significant numbers (> 100 to avoid noise)
                    if num > 100:
                        numbers.append(num)
                except (ValueError, AttributeError):
                    continue

        if not numbers:
            errors.append("No numeric values found in response")
            return False, errors

        # Check if any number falls within range
        found_in_range = False
        for num in numbers:
            if min_value is not None and num < min_value:
                continue
            if max_value is not None and num > max_value:
                continue
            found_in_range = True
            break

        if not found_in_range:
            if min_value is not None and max_value is not None:
                errors.append(f"No values found in range [{min_value:,.0f}, {max_value:,.0f}]. Found: {[f'{n:,.0f}' for n in numbers[:3]]}")
            elif min_value is not None:
                errors.append(f"No values found >= {min_value:,.0f}. Found: {[f'{n:,.0f}' for n in numbers[:3]]}")
            elif max_value is not None:
                errors.append(f"No values found <= {max_value:,.0f}. Found: {[f'{n:,.0f}' for n in numbers[:3]]}")

        return found_in_range, errors

    def run_single_test(
        self,
        test_case: Dict[str, Any],
        agent: LanduseAgent
    ) -> TestResult:
        """
        Run a single test case.

        Args:
            test_case: Test case configuration
            agent: Configured agent instance

        Returns:
            TestResult with outcome
        """
        result = TestResult(
            name=test_case.get('name', 'Unnamed test'),
            query=test_case['query'],
            passed=False,
            critical=test_case.get('critical', False)
        )

        start_time = time.time()

        try:
            # Execute query
            response = agent.query(test_case['query'])
            result.response_text = response
            result.execution_time = time.time() - start_time

            # Extract SQL (simplified - would need agent instrumentation)
            # For now, we'll check if response indicates data was retrieved

            # Validate SQL contains expected keywords
            if 'expected_sql_contains' in test_case:
                # Since we can't easily extract SQL from the current agent,
                # we'll check the response for indicators
                # In a real implementation, we'd instrument the agent
                result.warnings.append("SQL validation skipped (needs agent instrumentation)")

            # Validate response contains expected content
            if 'expected_response_contains' in test_case:
                passed, errors = self.validate_response_contains(
                    response,
                    test_case['expected_response_contains']
                )
                if not passed:
                    result.errors.extend(errors)

            # Check if query should be rejected (off-topic)
            if test_case.get('should_reject', False):
                # Check that response indicates the query is out of scope
                rejection_keywords = [
                    "land use", "RPA", "cannot", "outside", "scope",
                    "don't have", "not able", "unable to help", "can't answer"
                ]
                response_lower = response.lower() if response else ""
                if not any(keyword.lower() in response_lower for keyword in rejection_keywords):
                    result.errors.append("Off-topic query was not properly rejected")
                else:
                    result.warnings.append("Off-topic query correctly rejected")

            # Check numeric range validation
            if 'expected_numeric_range' in test_case:
                range_config = test_case['expected_numeric_range']
                passed, errors = self.validate_numeric_range(
                    response,
                    min_value=range_config.get('min'),
                    max_value=range_config.get('max')
                )
                if not passed:
                    result.errors.extend(errors)
                else:
                    result.warnings.append("Numeric validation passed")

            # Check if it should return data
            elif test_case.get('should_return_data', False):
                # Simple heuristic: check if response contains numbers/data
                if not re.search(r'\d+', response):
                    result.errors.append("Response doesn't appear to contain data")

            # Check graceful error handling
            if test_case.get('should_handle_gracefully', False):
                # If we got here without exception, it handled gracefully
                result.warnings.append("Graceful handling check: OK")

            # Determine overall pass/fail
            result.passed = len(result.errors) == 0

        except Exception as e:
            result.execution_time = time.time() - start_time

            # Check if error was expected
            if test_case.get('should_not_error', True):
                result.errors.append(f"Unexpected error: {str(e)}")
                if self.verbose:
                    result.errors.append(traceback.format_exc())
            else:
                # Error was expected
                result.passed = True
                result.warnings.append("Expected error occurred")

        return result

    def run_test_suite(
        self,
        version: Optional[str] = None,
        categories: Optional[List[str]] = None
    ) -> TestSuiteResult:
        """
        Run the complete test suite for a prompt version.

        Args:
            version: Prompt version to test (None for active version)
            categories: Specific categories to test (None for all)

        Returns:
            TestSuiteResult with all outcomes
        """
        version = version or self.prompt_manager.active_version

        self.console.print(f"\n[bold cyan]Testing Prompt Version: {version}[/bold cyan]")

        # Load benchmark queries
        all_queries = self.load_benchmark_queries()

        # Filter categories if specified
        if categories:
            all_queries = {k: v for k, v in all_queries.items() if k in categories}

        # Initialize agent with specific prompt version
        config = LanduseConfig()
        config.verbose = False  # Reduce noise during testing

        # Create agent (it will use the active version)
        if version != self.prompt_manager.active_version:
            # Temporarily switch version
            original_version = self.prompt_manager.active_version
            self.prompt_manager.set_active_version(version)
        else:
            original_version = None

        test_results = []
        total_tests = sum(len(tests) for tests in all_queries.values())

        try:
            with LanduseAgent(config) as agent:
                # Run tests by category
                for category, tests in all_queries.items():
                    self.console.print(f"\n[bold]{category.replace('_', ' ').title()}[/bold]")

                    for test_case in track(tests, description=f"Running {category}..."):
                        result = self.run_single_test(test_case, agent)
                        test_results.append(result)

                        if self.verbose:
                            self._print_test_result(result)

        finally:
            # Restore original version if we changed it
            if original_version:
                self.prompt_manager.set_active_version(original_version)

        # Calculate summary statistics
        passed = sum(1 for r in test_results if r.passed)
        failed = total_tests - passed
        critical_failures = sum(1 for r in test_results if not r.passed and r.critical)
        total_time = sum(r.execution_time for r in test_results)

        return TestSuiteResult(
            version=version,
            total_tests=total_tests,
            passed_tests=passed,
            failed_tests=failed,
            critical_failures=critical_failures,
            execution_time=total_time,
            test_results=test_results
        )

    def _print_test_result(self, result: TestResult):
        """Print detailed result for a single test."""
        status = result.status_emoji
        color = "green" if result.passed else "red"

        self.console.print(f"  {status} {result.name}: [{color}]{result.passed}[/{color}]")

        if result.errors:
            for error in result.errors:
                self.console.print(f"    [red]❌ {error}[/red]")

        if result.warnings:
            for warning in result.warnings:
                self.console.print(f"    [yellow]⚠️  {warning}[/yellow]")

    def print_summary(self, suite_result: TestSuiteResult):
        """
        Print a summary of test results.

        Args:
            suite_result: Complete test suite results
        """
        # Create summary table
        table = Table(title=f"Test Results for {suite_result.version}")
        table.add_column("Metric", style="cyan")
        table.add_column("Value", style="magenta")

        table.add_row("Total Tests", str(suite_result.total_tests))
        table.add_row("Passed", f"[green]{suite_result.passed_tests}[/green]")
        table.add_row("Failed", f"[red]{suite_result.failed_tests}[/red]")

        if suite_result.critical_failures > 0:
            table.add_row(
                "Critical Failures",
                f"[bold red]{suite_result.critical_failures}[/bold red]"
            )

        pass_color = "green" if suite_result.pass_rate >= 80 else "yellow" if suite_result.pass_rate >= 60 else "red"
        table.add_row("Pass Rate", f"[{pass_color}]{suite_result.pass_rate:.1f}%[/{pass_color}]")
        table.add_row("Execution Time", f"{suite_result.execution_time:.2f}s")

        self.console.print("\n")
        self.console.print(table)

        # Show failed tests
        if suite_result.failed_tests > 0:
            self.console.print("\n[bold red]Failed Tests:[/bold red]")
            for result in suite_result.test_results:
                if not result.passed:
                    self._print_test_result(result)

        # Overall status
        if suite_result.all_critical_passed and suite_result.pass_rate >= 80:
            self.console.print("\n[bold green]✅ Test suite PASSED[/bold green]")
        elif suite_result.all_critical_passed:
            self.console.print("\n[bold yellow]⚠️  Test suite PASSED with warnings[/bold yellow]")
        else:
            self.console.print("\n[bold red]❌ Test suite FAILED (critical tests failed)[/bold red]")

    def save_results(self, suite_result: TestSuiteResult, output_file: Optional[Path] = None):
        """
        Save test results to JSON file.

        Args:
            suite_result: Test results to save
            output_file: Output file path (auto-generated if None)
        """
        if output_file is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = Path(f"prompts/tests/results/{suite_result.version}_{timestamp}.json")

        output_file.parent.mkdir(parents=True, exist_ok=True)

        # Convert to JSON-serializable format
        results_dict = {
            'version': suite_result.version,
            'timestamp': suite_result.timestamp,
            'summary': {
                'total_tests': suite_result.total_tests,
                'passed_tests': suite_result.passed_tests,
                'failed_tests': suite_result.failed_tests,
                'critical_failures': suite_result.critical_failures,
                'pass_rate': suite_result.pass_rate,
                'execution_time': suite_result.execution_time
            },
            'tests': [
                {
                    'name': r.name,
                    'query': r.query,
                    'passed': r.passed,
                    'critical': r.critical,
                    'errors': r.errors,
                    'warnings': r.warnings,
                    'execution_time': r.execution_time
                }
                for r in suite_result.test_results
            ]
        }

        with open(output_file, 'w') as f:
            json.dump(results_dict, f, indent=2)

        self.console.print(f"\nResults saved to: {output_file}")


def main():
    """Main entry point for the test runner."""
    import argparse

    parser = argparse.ArgumentParser(description='Test prompt versions')
    parser.add_argument(
        '--version', '-v',
        help='Prompt version to test (default: active version)'
    )
    parser.add_argument(
        '--category', '-c',
        action='append',
        help='Test category to run (can specify multiple)'
    )
    parser.add_argument(
        '--verbose',
        action='store_true',
        help='Show detailed output'
    )
    parser.add_argument(
        '--save-results',
        action='store_true',
        help='Save results to JSON file'
    )

    args = parser.parse_args()

    # Create and run test runner
    runner = PromptTestRunner(verbose=args.verbose)

    try:
        results = runner.run_test_suite(
            version=args.version,
            categories=args.category
        )

        runner.print_summary(results)

        if args.save_results:
            runner.save_results(results)

        # Exit with appropriate code
        sys.exit(0 if results.all_critical_passed else 1)

    except Exception as e:
        runner.console.print(f"[bold red]Error: {e}[/bold red]")
        if args.verbose:
            runner.console.print(traceback.format_exc())
        sys.exit(2)


if __name__ == "__main__":
    main()