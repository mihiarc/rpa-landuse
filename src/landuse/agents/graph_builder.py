"""LangGraph workflow construction extracted from monolithic agent class."""

from typing import Any, Dict, List, Optional, Union

from langchain_core.language_models import BaseChatModel
from langchain_core.messages import AIMessage, BaseMessage, HumanMessage
from langchain_core.tools import BaseTool
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import END, StateGraph
from langgraph.prebuilt import ToolNode
from rich.console import Console

from landuse.agents.state import AgentState
from landuse.config.landuse_config import LanduseConfig
from landuse.core.app_config import AppConfig


class GraphBuilder:
    """
    Constructs and manages LangGraph workflows.
    
    Extracted from the monolithic LanduseAgent class to follow Single Responsibility Principle.
    Handles graph construction, node creation, and workflow orchestration.
    """

    def __init__(
        self, 
        config: Union[LanduseConfig, AppConfig], 
        llm: BaseChatModel, 
        tools: List[BaseTool],
        system_prompt: str,
        console: Optional[Console] = None
    ):
        """
        Initialize graph builder.
        
        Args:
            config: Configuration object (AppConfig or legacy LanduseConfig)
            llm: Language model instance
            tools: List of available tools
            system_prompt: System prompt for the agent
            console: Rich console for logging (optional)
        """
        if isinstance(config, AppConfig):
            self.app_config = config
            self.config = self._convert_to_legacy_config(config)
        else:
            self.config = config
            self.app_config = None
            
        self.llm = llm
        self.tools = tools
        self.system_prompt = system_prompt
        self.console = console or Console()
        self.memory = MemorySaver()

    def build_graph(self) -> StateGraph:
        """
        Build the main LangGraph state graph.
        
        Returns:
            Compiled StateGraph ready for execution
        """
        # Create the graph
        workflow = StateGraph(AgentState)

        # Add nodes
        workflow.add_node("agent", self._agent_node)
        workflow.add_node("tools", ToolNode(self.tools))
        workflow.add_node("analyzer", self._analyzer_node)
        workflow.add_node("human_review", self._human_review_node)

        # Set entry point
        workflow.set_entry_point("agent")

        # Add conditional edges
        workflow.add_conditional_edges(
            "agent",
            self._should_continue,
            {
                "tools": "tools",
                "analyzer": "analyzer",
                "human_review": "human_review",
                "end": END
            }
        )

        # Add edges from tools back to agent
        workflow.add_edge("tools", "agent")
        workflow.add_edge("analyzer", "agent")
        workflow.add_edge("human_review", "agent")

        # Compile with memory if enabled
        if self.config.enable_memory:
            return workflow.compile(checkpointer=self.memory)
        else:
            return workflow.compile()

    def _agent_node(self, state: AgentState) -> Dict[str, Any]:
        """Main agent node that decides next action."""
        messages = state["messages"]

        # Ensure we have proper message types
        if not messages:
            messages = []
        
        # Add system prompt as first message if needed
        has_system = any(
            isinstance(m, (HumanMessage, AIMessage)) and 
            self.system_prompt[:50] in str(m.content) 
            for m in messages[:1]
        )
        
        if not has_system:
            messages = [HumanMessage(content=self.system_prompt)] + messages

        # Get LLM response with tools bound
        response = self.llm.bind_tools(self.tools).invoke(messages)

        # Update state with new message
        return {
            "messages": messages + [response],
            "iteration_count": state.get("iteration_count", 0) + 1
        }

    def _analyzer_node(self, state: AgentState) -> Dict[str, Any]:
        """Analyzer node for providing insights on query results."""
        messages = state["messages"]

        # Extract recent query results
        recent_results = self._extract_recent_results(messages)

        if recent_results:
            # Create analysis prompt
            analysis_prompt = f"""Based on these query results, provide key insights:

Results: {recent_results}

Focus on:
1. Key trends or patterns
2. Implications for land use planning
3. Comparison with historical patterns
4. Recommendations or areas for further investigation
"""

            # Get analysis
            analysis = self.llm.invoke([
                {"role": "system", "content": "You are a land use science expert."},
                {"role": "user", "content": analysis_prompt}
            ])

            return {"messages": messages + [analysis]}

        return {"messages": messages}

    def _human_review_node(self, state: AgentState) -> Dict[str, Any]:
        """Human-in-the-loop node for complex queries."""
        # In production, this would integrate with a UI
        # For now, we'll auto-approve
        if self.console:
            self.console.print("[yellow]Human review requested (auto-approved)[/yellow]")
        return {"messages": state["messages"]}

    def _should_continue(self, state: AgentState) -> str:
        """Decide next step in the workflow."""
        messages = state["messages"]
        last_message = messages[-1]

        # Check iteration limit
        if state.get("iteration_count", 0) >= self.config.max_iterations:
            return "end"

        # Check if tools were called
        if hasattr(last_message, "tool_calls") and last_message.tool_calls:
            return "tools"

        # Check if analysis is needed
        if self._needs_analysis(messages):
            return "analyzer"

        # Check if human review is needed (for sensitive queries)
        if self._needs_human_review(messages):
            return "human_review"

        # Otherwise, we're done
        return "end"

    def _needs_analysis(self, messages: List[BaseMessage]) -> bool:
        """Determine if results need analysis."""
        # Check if recent messages contain query results
        for msg in messages[-3:]:
            if isinstance(msg, AIMessage) and "SELECT" in str(msg.content).upper():
                return True
        return False

    def _needs_human_review(self, messages: List[BaseMessage]) -> bool:
        """Determine if human review is needed."""
        # Check for sensitive operations
        sensitive_keywords = ["DELETE", "UPDATE", "DROP", "TRUNCATE"]
        last_message = str(messages[-1].content).upper()

        return any(keyword in last_message for keyword in sensitive_keywords)

    def _extract_recent_results(self, messages: List[BaseMessage]) -> Optional[str]:
        """Extract recent query results from messages."""
        for msg in reversed(messages[-5:]):
            content = str(msg.content)
            if "rows returned" in content or "â”‚" in content:
                return content
        return None

    def create_subgraph(self, name: str, specialized_tools: List[BaseTool]) -> StateGraph:
        """
        Create a specialized subgraph for complex workflows.

        This follows the 2025 pattern of using subgraphs for modularity.

        Args:
            name: Name of the subgraph
            specialized_tools: Tools specific to this subgraph

        Returns:
            Compiled subgraph
        """
        subgraph = StateGraph(AgentState)

        # Add specialized nodes
        subgraph.add_node("specialized_agent", self._agent_node)
        subgraph.add_node("specialized_tools", ToolNode(specialized_tools))

        # Set up flow
        subgraph.set_entry_point("specialized_agent")
        subgraph.add_edge("specialized_agent", "specialized_tools")
        subgraph.add_edge("specialized_tools", END)

        return subgraph.compile()

    def _convert_to_legacy_config(self, app_config: AppConfig) -> LanduseConfig:
        """Convert AppConfig to legacy LanduseConfig for backward compatibility."""
        # Create legacy config bypassing validation for now
        from landuse.config.landuse_config import LanduseConfig
        
        # Create instance without validation to avoid API key issues during conversion
        legacy_config = object.__new__(LanduseConfig)
        
        # Map database settings
        legacy_config.db_path = app_config.database.path
        
        # Map LLM settings 
        legacy_config.model = app_config.llm.model_name  # Note: model_name in AppConfig vs model in legacy
        legacy_config.temperature = app_config.llm.temperature
        legacy_config.max_tokens = app_config.llm.max_tokens
        
        # Map agent execution settings
        legacy_config.max_iterations = app_config.agent.max_iterations
        legacy_config.max_execution_time = app_config.agent.max_execution_time
        legacy_config.max_query_rows = app_config.agent.max_query_rows
        legacy_config.default_display_limit = app_config.agent.default_display_limit
        
        # Map debugging settings
        legacy_config.debug = app_config.logging.level == 'DEBUG'
        legacy_config.enable_memory = app_config.agent.enable_memory
        
        return legacy_config