name: Performance Testing

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  workflow_dispatch:  # Allow manual triggering
  schedule:
    # Run performance tests weekly on Sundays at 3 AM UTC
    - cron: '0 3 * * 0'

env:
  PYTHON_VERSION: "3.12"

jobs:
  benchmark-duckdb:
    runs-on: ubuntu-latest
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Install uv
      uses: astral-sh/setup-uv@v4
      with:
        enable-cache: true
        
    - name: Install dependencies
      run: uv sync --all-extras --dev
      
    - name: Create test environment
      run: |
        mkdir -p data/processed data/raw config
        echo "LANDUSE_MODEL=gpt-4o-mini" > config/.env
        
    - name: Run DuckDB bulk loading benchmarks
      run: |
        uv run python -m landuse.converters.performance_benchmark \
          --records 1000 10000 50000 \
          --output performance-report.md
          
    - name: Upload performance report
      uses: actions/upload-artifact@v4
      with:
        name: duckdb-performance-report
        path: performance-report.md
        
    - name: Create performance summary
      run: |
        echo "# DuckDB Performance Summary" > perf-summary.md
        echo "" >> perf-summary.md
        if [ -f performance-report.md ]; then
          # Extract key metrics from the report
          grep -A 10 "## Executive Summary" performance-report.md >> perf-summary.md || true
        fi
        
    - name: Comment performance results on PR
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          if (fs.existsSync('perf-summary.md')) {
            const summary = fs.readFileSync('perf-summary.md', 'utf8');
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: `## ðŸš€ Performance Test Results\n\n${summary}`
            });
          }

  memory-profiling:
    runs-on: ubuntu-latest
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Install uv
      uses: astral-sh/setup-uv@v4
      
    - name: Install dependencies with profiling tools
      run: |
        uv sync --all-extras --dev
        uv add memory-profiler psutil
        
    - name: Create test environment
      run: |
        mkdir -p data/processed data/raw config
        echo "LANDUSE_MODEL=gpt-4o-mini" > config/.env
        touch data/processed/landuse_analytics.duckdb
        
    - name: Run memory profiling on critical operations
      run: |
        # Create memory profiling script
        cat > memory_profile_test.py << 'EOF'
        #!/usr/bin/env python3
        """Memory profiling for critical operations"""
        
        import sys
        sys.path.append('src')
        
        from memory_profiler import profile
        import pandas as pd
        import tempfile
        from pathlib import Path
        
        from landuse.converters.bulk_loader import DuckDBBulkLoader
        from landuse.connections.duckdb_connection import DuckDBConnection
        
        @profile
        def test_bulk_loading_memory():
            """Test memory usage of bulk loading operations"""
            # Create test data
            data = {
                'id': range(10000),
                'value': range(10000),
                'category': ['A', 'B', 'C'] * 3334
            }
            df = pd.DataFrame(data)
            
            # Test bulk loading
            with tempfile.NamedTemporaryFile(suffix='.duckdb', delete=False) as f:
                db_path = f.name
            
            try:
                with DuckDBBulkLoader(db_path) as loader:
                    # Create test table
                    with loader.connection() as conn:
                        conn.execute("""
                            CREATE TABLE test_table (
                                id INTEGER,
                                value INTEGER, 
                                category VARCHAR
                            )
                        """)
                    
                    # Load data
                    stats = loader.bulk_load_dataframe(df, "test_table")
                    print(f"Loaded {stats.processed_records} records")
                    
            finally:
                Path(db_path).unlink(missing_ok=True)
        
        @profile  
        def test_connection_memory():
            """Test memory usage of database connections"""
            # Create connection multiple times
            for i in range(100):
                try:
                    conn = DuckDBConnection("test_db")
                    # Simulate some work
                    result = [1, 2, 3, 4, 5] * 1000
                    del result
                except:
                    pass  # Expected to fail without actual DB
        
        if __name__ == "__main__":
            print("Running memory profiling tests...")
            test_bulk_loading_memory()
            test_connection_memory()
            print("Memory profiling complete!")
        EOF
        
        # Run profiling
        uv run python memory_profile_test.py > memory-profile.txt 2>&1 || true
        
    - name: Upload memory profiling results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: memory-profile
        path: memory-profile.txt

  load-testing:
    runs-on: ubuntu-latest
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Install uv
      uses: astral-sh/setup-uv@v4
      
    - name: Install dependencies
      run: uv sync --all-extras --dev
      
    - name: Create test environment
      run: |
        mkdir -p data/processed data/raw config
        echo "LANDUSE_MODEL=gpt-4o-mini" > config/.env
        echo "TEMPERATURE=0.1" >> config/.env
        echo "MAX_TOKENS=1000" >> config/.env
        echo "OPENAI_API_KEY=test-key" >> config/.env
        # Create minimal test database (remove any existing file first)
        rm -f data/processed/landuse_analytics.duckdb
        uv run python -c "
        import duckdb
        conn = duckdb.connect('data/processed/landuse_analytics.duckdb')
        conn.execute('CREATE TABLE test_table (id INTEGER, name VARCHAR)')
        conn.execute(\"INSERT INTO test_table VALUES (1, 'test')\")
        conn.close()
        print('âœ… Test database created successfully')
        "
        
    - name: Run agent load testing
      run: |
        # Create load testing script
        cat > load_test.py << 'EOF'
        #!/usr/bin/env python3
        """Load testing for landuse agents"""
        
        import sys
        import time
        import concurrent.futures
        import statistics
        from typing import List
        
        sys.path.append('src')
        
        def simulate_query_load(num_queries: int = 10) -> List[float]:
            """Simulate multiple concurrent queries"""
            from landuse.agents.landuse_agent import LanduseAgent
            
            try:
                from landuse.config.landuse_config import LanduseConfig
                from unittest.mock import patch
                
                # Bypass validation like in tests
                with patch('landuse.config.landuse_config.LanduseConfig.__post_init__', return_value=None):
                    config = LanduseConfig(db_path='data/processed/landuse_analytics.duckdb')
                
                agent = LanduseAgent(config)
            except Exception as e:
                print(f"Could not create agent: {e}")
                return [0.0] * num_queries
            
            def single_query() -> float:
                """Execute a single query and return timing"""
                start_time = time.time()
                try:
                    # Simple query that should work with minimal DB
                    result = agent.process_query("SELECT COUNT(*) FROM test_table")
                    return time.time() - start_time
                except Exception as e:
                    print(f"Query failed: {e}")
                    return time.time() - start_time  # Return time even on failure
            
            # Run queries concurrently
            response_times = []
            with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
                futures = [executor.submit(single_query) for _ in range(num_queries)]
                for future in concurrent.futures.as_completed(futures):
                    response_times.append(future.result())
            
            return response_times
        
        def main():
            print("Starting load testing...")
            
            # Test different load levels
            for num_queries in [5, 10, 20]:
                print(f"\nTesting with {num_queries} concurrent queries...")
                
                response_times = simulate_query_load(num_queries)
                
                if response_times:
                    avg_time = statistics.mean(response_times)
                    max_time = max(response_times)
                    min_time = min(response_times)
                    
                    print(f"Average response time: {avg_time:.3f}s")
                    print(f"Max response time: {max_time:.3f}s") 
                    print(f"Min response time: {min_time:.3f}s")
                    
                    # Check for performance regressions
                    if avg_time > 5.0:  # 5 second threshold
                        print(f"âš ï¸ Warning: Average response time ({avg_time:.3f}s) exceeds threshold")
                    
                    if max_time > 10.0:  # 10 second threshold
                        print(f"âš ï¸ Warning: Max response time ({max_time:.3f}s) exceeds threshold")
        
        if __name__ == "__main__":
            main()
        EOF
        
        # Run load testing with environment variables
        export OPENAI_API_KEY=test-key
        export LANDUSE_MODEL=gpt-4o-mini
        uv run python load_test.py > load-test-results.txt 2>&1 || true
        
    - name: Upload load testing results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: load-test-results
        path: load-test-results.txt
        
    - name: Check for performance regressions
      run: |
        if grep -q "Warning:" load-test-results.txt; then
          echo "âš ï¸ Performance regression detected!"
          grep "Warning:" load-test-results.txt
          exit 1
        else
          echo "âœ… No performance regressions detected"
        fi

  performance-summary:
    needs: [benchmark-duckdb, memory-profiling, load-testing]
    runs-on: ubuntu-latest
    if: always()
    steps:
    - name: Download all performance artifacts
      uses: actions/download-artifact@v4
      
    - name: Create comprehensive performance report
      run: |
        echo "# Performance Testing Report" > performance-summary.md
        echo "" >> performance-summary.md
        echo "Generated: $(date -u)" >> performance-summary.md
        echo "" >> performance-summary.md
        
        # DuckDB Performance
        echo "## DuckDB Bulk Loading Performance" >> performance-summary.md
        if [ -f duckdb-performance-report/performance-report.md ]; then
          # Extract executive summary
          sed -n '/## Executive Summary/,/## Detailed Results/p' duckdb-performance-report/performance-report.md | head -n -1 >> performance-summary.md
        else
          echo "DuckDB benchmarks not available" >> performance-summary.md
        fi
        echo "" >> performance-summary.md
        
        # Memory Profiling
        echo "## Memory Usage Analysis" >> performance-summary.md
        if [ -f memory-profile/memory-profile.txt ]; then
          echo "```" >> performance-summary.md
          # Extract memory usage summary (first 20 lines)
          head -20 memory-profile/memory-profile.txt >> performance-summary.md
          echo "```" >> performance-summary.md
        else
          echo "Memory profiling results not available" >> performance-summary.md
        fi
        echo "" >> performance-summary.md
        
        # Load Testing
        echo "## Load Testing Results" >> performance-summary.md
        if [ -f load-test-results/load-test-results.txt ]; then
          echo "```" >> performance-summary.md
          cat load-test-results/load-test-results.txt >> performance-summary.md
          echo "```" >> performance-summary.md
        else
          echo "Load testing results not available" >> performance-summary.md
        fi
        echo "" >> performance-summary.md
        
        # Recommendations
        echo "## Performance Recommendations" >> performance-summary.md
        echo "" >> performance-summary.md
        echo "1. **DuckDB Operations**: Use bulk COPY for datasets >10K records" >> performance-summary.md
        echo "2. **Memory Management**: Monitor memory usage during large operations" >> performance-summary.md
        echo "3. **Concurrent Access**: Limit concurrent database connections" >> performance-summary.md
        echo "4. **Query Optimization**: Use appropriate indexes and LIMIT clauses" >> performance-summary.md
        
    - name: Upload comprehensive performance report
      uses: actions/upload-artifact@v4
      with:
        name: performance-summary
        path: performance-summary.md